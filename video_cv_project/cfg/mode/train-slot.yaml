# @package _global_
# Config for train mode.

defaults:
  - slot-base
  - override /data: kinetics400
  - override /data/transform: base
  - override /model: slot

mode: train

train:
  # Hardcode to 1; Use `clips_per_vid` to control epochs instead.
  epochs: 1

  log_every: 50
  save_every: 1000

  dryrun: false

  # TODO: Optimizer/scheduler can be its own sub-config.
  optimizer:
    _target_: torch.optim.Adam
    lr: ${hparam.general.lr}

  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    max_lr: ${..optimizer.lr}
    # `total_steps` is added in at runtime.
    total_steps: ${total_steps}
    pct_start: ${hparam.general.pct_start}

  clip_gradnorm: ${hparam.general.clip_gradnorm}

data:
  cache_path: /home/jovyan/downloads/kinetics400-train.pt
  batch_size: ${hparam.general.batch_size}
  num_workers: 32
  clips_per_vid: ${hparam.general.epochs}
  dataset:
    frame_rate: ${hparam.general.framerate}
    frames_per_clip: ${hparam.general.frames_per_clip}
    step_between_clips: ${.frames_per_clip}
    split: train

  transform:
    patch_func:
      # Done on CPU, cannot use too high batch size.
      device: cpu
      batch_size: 1
