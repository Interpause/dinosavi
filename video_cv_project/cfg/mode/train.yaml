# @package _global_
# Config for train mode.

defaults:
  - override /data: kinetics400

mode: train

train:
  epochs: 1

  log_every: 20
  save_every: 500

  # TODO: Optimizer/scheduler can be its own sub-config.
  # optimizer:
  #   # RAdam has built-in warm-up & is useful for prototyping.
  #   _target_: torch.optim.RAdam
  #   lr: 0.0002
  #   # weight_decay: 0.0001
  #
  # scheduler:
  #   _target_: torch.optim.lr_scheduler.MultiStepLR
  #   milestones: [0.8, 0.9, 0.95]
  #   gamma: 0.5

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0002

  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    max_lr: ${..optimizer.lr}
    total_steps: ${total_steps}
    pct_start: 0.05

model:
  edge_dropout: 0.1
  feat_dropout: 0.0
  temperature: 0.05
